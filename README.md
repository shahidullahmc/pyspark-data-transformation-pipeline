# pyspark-data-transformation-pipeline
A PySpark project demonstrating data transformation, integration, and writing outputs to Hive and HDFS.

# PySpark Data Transformation & Integration Pipeline
---
## Project Overview
This project demonstrates a complete data transformation and integration workflow using **PySpark**.  
You will work with two datasets, apply a series of transformations, join them into a unified view, and write the final output to both **Hive** and **HDFS**.

The project reflects real-world data engineering practices, including:
- Schema alignment
- Column-level transformations
- Data cleaning
- Joining datasets
- Writing to distributed storage systems

## Objectives
- Load two datasets into Spark DataFrames  
- Perform column transformations:
  - Add new columns
  - Rename existing columns
  - Drop unnecessary fields  
- Join the datasets using appropriate keys  
- Write the final integrated dataset to:
  - **Hive warehouse** (managed table)
  - **HDFS** (as Parquet or CSV)

## Technologies Used
- **PySpark**
- **Apache Spark SQL**
- **Hive Warehouse**
- **HDFS**
- **Parquet / CSV**
- **Python 3.7.12**

---

## ðŸ“‚ Project Structure
